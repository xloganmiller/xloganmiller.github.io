---
title: "LM_DDS_Project2"
author: "Logan Miller"
date: "8/4/2021"
output: 
  html_document:
    toc: true #specifies output settings for output types, this one specifies no table of contents
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Link to video: https://youtu.be/2o8DzHAcOY8

## Executive Summary

The goal of this analysis is to provide insights regarding attrition, monthly income, and other metrics. The data and subsequent analysis suggest, but do not necessarily prove, the following:

* Sales representatives have the highest attrition rate by far at 45%
  + This is to be expected since sales is a highly volatile division in general but still something to look at
* Longer commutes appears to lead to higher attrition
  + When opening locations, try to position it as close to potential employees as possible
* It does not appear like non-exempt employees have a lower job satisfaction than exempt employees
* Out of logistic regression, knn, and naive bayes, naive bayes was the best at classify attrition
* With the given variables, a linear regression with stepwise selection achieved an impressive adjusted R-squared of 95%


## Import Relevant Packages
```{r, message = FALSE}

library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(caret)
library(e1071)
library(car)
library(aod)
library(olsrr)
library(psych)
library(class)
```

## Import Data

```{r}
data = read.csv(file = "https://raw.githubusercontent.com/xloganmiller/CaseStudy2DDS/main/CaseStudy2-data.csv",
                         stringsAsFactors = T,
                         header = T)

colnames(data)
```
## Transforming Data 
```{r}
data$JobSatisfaction = factor(data$JobSatisfaction)
data$EnvironmentSatisfaction = factor(data$EnvironmentSatisfaction)
data$JobRole = factor(data$JobRole)
data$StockOptionLevel = factor(data$StockOptionLevel)
data$PerformanceRating = factor(data$PerformanceRating)
data$RelationshipSatisfaction = factor(data$RelationshipSatisfaction)
data$WorkLifeBalance = factor(data$WorkLifeBalance)
data$Education = factor(data$Education)
data$JobInvolvement = factor(data$JobInvolvement)
data$JobLevel = factor(data$JobLevel)

filtered_data = data %>% dplyr::select(-EmployeeCount, -ID, -Over18, -EmployeeNumber, -StandardHours)

numeric_columns = filtered_data %>% select_if(is.numeric) %>% colnames()

scaled_data = filtered_data

scaled_data[, numeric_columns] = scale(scaled_data[, numeric_columns])

```

## Visualizing Continuous Variables
```{r, fig.height=10, fig.width=13, fig.align='center'}
filtered_data %>% 
  select_if(is.numeric) %>%
  gather() %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 15, fill = 'dark green', color = 'black') +
  facet_wrap(~key, scales = 'free') +
  theme_economist()
  
```

Variables have different distributions so we will need to use the scaled data in KNN


## Key averages by job role
```{r}
data = data %>% mutate(AttritionValue = ifelse(Attrition == 'Yes', 1, 0))

data %>%
  dplyr::select(JobRole, Attrition, JobSatisfaction) %>%
  mutate(Turnover = ifelse(Attrition == 'Yes', 1, 0)) %>%
  group_by(JobRole) %>%
  summarise(
    TurnoverCount = sum(Turnover),
    EmployeeCount = n(),
    TurnoverPercent = round(sum(Turnover) / n(), 4),
    AverageJobSat = mean(as.numeric(JobSatisfaction))
  ) %>%
  arrange(-AverageJobSat)

data %>%
  dplyr::select(Attrition, Gender, JobLevel) %>%
  filter(Attrition == 'Yes') %>%
  group_by(Gender) %>%
  summarise(
    TurnoverCount = n(),
    EmployeeCount = 140,
    TurnoverPercent = round(n() / 140, 4),
    AverageJobLevel = mean(as.numeric(JobLevel))
  )

data %>%
  dplyr::select(Gender) %>%
  group_by(Gender) %>%
  summarise(
    count = n()
  )

data %>%
  dplyr::select(JobRole, Attrition) %>%
  mutate(Turnover = ifelse(Attrition == 'Yes', 1, 0)) %>%
  group_by(JobRole) %>%
  summarise(
    TurnoverPercent = round(sum(Turnover) / n(), 4),
  ) %>%
  mutate(JobRole = fct_reorder(JobRole, TurnoverPercent)) %>%
  ggplot(aes(x = JobRole, y = TurnoverPercent)) +
  geom_col(fill = 'dark green', color = 'black')+
  coord_flip() +
  theme_economist()+
  labs( x = '', y = 'Attrition Rate',title = 'Attrition Rate by Job Role')

summary(data$Attrition)
```
Research Director lowest average job satisfaction. Sales Representatives have extremely high turnover.

##Split into train and test sets for regressions

```{r}
set.seed(66)

sample_size = floor(.70 * nrow(filtered_data))
train_index = sample(seq_len(nrow(filtered_data)), size = sample_size)
train_df = filtered_data[train_index, ]
test_df = filtered_data[-train_index, ]

```

## Logistic Regression
```{r warning = FALSE, message=FALSE, results='hide'}
options(warn = 1)

logit_model = glm(
  relevel(Attrition, ref = "Yes") ~ .,
  data = train_df,
  family = 'binomial'
)

step_logit_model = step(logit_model, trace = 0, direction = 'both')

summary(step_logit_model)

options(warn = 1)

# aod::wald.test(b = coef(logit_model), Sigma = vcov(logit_model), Terms = 5:8)
```

```{r}
options(scipen = 100)
step_logit_coeffs = data.frame(coeffs = round(exp(step_logit_model$coefficients), 3))
step_logit_coeffs$name = rownames(step_logit_coeffs)

step_logit_coeffs
```


```{r}

test_probs = predict(step_logit_model, test_df, type = 'response')

test_preds = ifelse(test_probs > 0.5, 'No', 'Yes')


cm = caret::confusionMatrix(table(test_preds, test_df$Attrition))

cm

```
Logit regression does a decent job of predicting attrition. However, it struggles more with correctly predicting employees who left their jobs than vice versa.

## KNN Clustering
```{r warning = FALSE, message=FALSE}
factor_names = filtered_data %>% dplyr::select(-Attrition)  %>% select_if(is.factor) %>% colnames()



knn_scaled_df = scaled_data 


for(col_name in factor_names){
  if(length(levels(knn_scaled_df[, col_name])) == 2){
    knn_scaled_df[, col_name] = dummy.code(knn_scaled_df[, col_name])
  } else{
    
    dummy_df = as.data.frame(psych::dummy.code(knn_scaled_df[, col_name]))
    
    colnames(dummy_df) = paste(col_name, colnames(dummy_df))
    
    knn_scaled_df = cbind(knn_scaled_df, dummy_df)
    
    knn_scaled_df = knn_scaled_df %>% dplyr::select(-col_name)
    
    
  }
}



knn_test_comparision = data.frame(accuracy = numeric(100)
                              , sensitivity = numeric(100)
                              , specificity = numeric(100))



train_data = knn_scaled_df[train_index,]
test_data = knn_scaled_df[-train_index,]
  
knn_test_comparision = data.frame(k = 1:20
                              ,accuracy = numeric(20)
                              , sensitivity = numeric(20)
                              , specificity = numeric(20))

set.seed(69)

for(i in 1:20) {


  loop_knn_model = knn(
    train = train_data %>% dplyr::select(-Attrition),
    test = test_data %>% dplyr::select(-Attrition),
    cl = train_data$Attrition,
    k = i,
    prob = TRUE
  )
  

  cf = confusionMatrix(table(loop_knn_model, test_data$Attrition))

  accuracy = round(cf$overall[1], 4)
  sensitivity = round(cf$byClass[1], 4)
  specificity = round(cf$byClass[2], 4)

  knn_test_comparision[i, c(2,3,4)] = c(accuracy,
                                         sensitivity,
                                         specificity)
}


knn_test_comparision
```
Decent overall accuracy but poor specifity, presumably due to KNN's shortcomings when it comes to categorical variables


## NaiveBayes
```{r}
nb_train = scaled_data[train_index,]
nb_test = scaled_data[-train_index,]

nb_model = naiveBayes(Attrition ~. , data = nb_train)


nb_preds = predict(nb_model, nb_test, type = 'class')

caret::confusionMatrix(table(nb_preds, nb_test$Attrition))

nb_test_comparision = data.frame(accuracy = numeric(20)
                              , sensitivity = numeric(20)
                              , specificity = numeric(20))
for(i in 1:20) {
  
  nb_train_index = sample(seq_len(nrow(filtered_data)), size = sample_size)
  nb_loop_train = scaled_data[nb_train_index,]
  nb_loop_test = scaled_data[-nb_train_index,]
  
  loop_nb_model = naiveBayes(Attrition ~. , data = nb_loop_train)
  loop_nb_preds = predict(nb_model, nb_loop_test, type = 'class')

  cf = confusionMatrix(table(loop_nb_preds, nb_loop_test$Attrition))
  
  cf

  accuracy = round(cf$overall[1], 4)
  sensitivity = round(cf$byClass[1], 4)
  specificity = round(cf$byClass[2], 4)

  nb_test_comparision[i, c(1,2,3)] = c(accuracy,
                                         sensitivity,
                                         specificity)
}

nb_test_comparision %>% gather() %>% group_by(key) %>% summarise(average = mean(value))

```
Similar accuracy to the KNN model but the NB model is much more well balanced and the difference between the sensitivity and specifity averages over 20 iterations is much smaller.

```{r, include=FALSE, eval=FALSE}
other_new_data = read.csv(file = "https://raw.githubusercontent.com/xloganmiller/CaseStudy2DDS/main/CaseStudy2CompSet%20No%20Attrition.csv",
                          stringsAsFactors = T,
                          header = T)

other_new_data$JobSatisfaction = factor(other_new_data$JobSatisfaction)
other_new_data$EnvironmentSatisfaction = factor(other_new_data$EnvironmentSatisfaction)
other_new_data$JobRole = factor(other_new_data$JobRole)
other_new_data$StockOptionLevel = factor(other_new_data$StockOptionLevel)
other_new_data$PerformanceRating = factor(other_new_data$PerformanceRating)
other_new_data$RelationshipSatisfaction = factor(other_new_data$RelationshipSatisfaction)
other_new_data$WorkLifeBalance = factor(other_new_data$WorkLifeBalance)
other_new_data$Education = factor(other_new_data$Education)
other_new_data$JobInvolvement = factor(other_new_data$JobInvolvement)
other_new_data$JobLevel = factor(other_new_data$JobLevel)


numeric_columns = other_new_data %>% select_if(is.numeric) %>% colnames()

scaled_other_new_data = other_new_data

scaled_other_new_data[, numeric_columns] = scale(scaled_other_new_data[, numeric_columns])

real_nb_pred = predict(nb_model, scaled_other_new_data, type = 'class')

other_new_data$Attrition = real_nb_pred




write.csv(
  other_new_data
  ,
  'C:/Users/L/Downloads/new_data.csv'
  ,
  row.names = FALSE
)
```

## Linear Regression to Predict Salary
```{r}
linear_model = lm(MonthlyIncome ~. , data = train_df)

step_linear_model = step(linear_model, trace = 0)

summary(step_linear_model)

step_lm_pred = predict(step_linear_model, test_df)

mean(step_lm_pred)

# Does the R-squared remain high for the test set
cor(step_lm_pred, test_df$MonthlyIncome) ^ 2

RMSE(step_lm_pred, test_df$MonthlyIncome)

caret::varImp(step_logit_model)
```


```{r, include=FALSE, eval=FALSE}
new_data = read.csv(file = "https://raw.githubusercontent.com/xloganmiller/CaseStudy2DDS/main/CaseStudy2CompSet%20No%20Salary.csv",
                         stringsAsFactors = T,
                         header = T)

new_data$JobSatisfaction = factor(new_data$JobSatisfaction)
new_data$EnvironmentSatisfaction = factor(new_data$EnvironmentSatisfaction)
new_data$JobRole = factor(new_data$JobRole)
new_data$StockOptionLevel = factor(new_data$StockOptionLevel)
new_data$PerformanceRating = factor(new_data$PerformanceRating)
new_data$RelationshipSatisfaction = factor(new_data$RelationshipSatisfaction)
new_data$WorkLifeBalance = factor(new_data$WorkLifeBalance)
new_data$Education = factor(new_data$Education)
new_data$JobInvolvement = factor(new_data$JobInvolvement)
new_data$JobLevel = factor(new_data$JobLevel)


numeric_columns = new_data %>% select_if(is.numeric) %>% colnames()

scaled_new_data = new_data

scaled_new_data[, numeric_columns] = scale(scaled_new_data[, numeric_columns])

real_step_lm_pred = predict(step_linear_model, scaled_new_data)

new_data$MonthlyIncome = real_step_lm_pred

write.csv(new_data %>% dplyr::select(ID, MonthlyIncome)
          , 'C:/Users/L/Downloads/new_data.csv'
          , row.names = FALSE)
```

Using stepwise selection, the linear regression model was able to achieve an impressive 95.24% r-squared and the r-squared for the test set was pretty much the same, suggesting that even though the r-squared is very high, the model is not overfitting. The adjusted r-squared is essentially the same as the r-squared, so the model is able to explain a significant amount of variance with minimal variables.

## Testing if employees who left had a different average commute distance than employees who haven't left

```{r}
data %>% 
  ggplot(aes(x = DistanceFromHome)) +
  geom_histogram(bins = 10, color = 'black', fill = 'dark green') +
  facet_wrap(~Attrition, scales = 'free') +
  labs(title = 'Distribution of Commute Distance by Attrition', x = 'Distance From Home') +
  theme_economist() 

data %>% 
  ggplot(aes(x = DistanceFromHome)) +
  geom_boxplot(color = 'black', fill = 'dark green') +
  facet_wrap(~Attrition) +
  labs(title = 'Boxplot', x = 'Distance From Home') +
  theme_economist() 


# Checking sample sizes and standard deviations 

data %>% 
  group_by(Attrition) %>% 
  summarise(stdev = sd(DistanceFromHome), average = mean(DistanceFromHome), count = n())

# Formal test for assuming same standard deviations

leveneTest(DistanceFromHome ~ Attrition,data = data)

att_ttest = t.test(DistanceFromHome ~ Attrition, var.equal = FALSE, alternative = 'less',data = data)


att_ttest

```
 Due to the large sample sizes, the Central Limit Thereom is met and because we cannot assume equal variances, I proceeded to conduct a Welch's two sample t-test. Based on the results of the test, we are 95% that the true difference in means is not equal 0. Furthermore, since the mean travel distance for those who left is larger than those who didn't, we can say with 95% that the mean travel distance for employees who left their job is greater than the mean travel distance of employees who remain at their job, p-value = 0.01641.

## Permutation Test for comparing Job Satisfaction for employees who work overtime vs those who don't
```{r, message=FALSE, warning=FALSE}
permutation_test = function(df,categorical_column, result_column, number_permutations = 10000) {
  
  perm_data = df %>% dplyr::select(categorical_column, result_column)
  perm_data[, result_column] = as.numeric(perm_data[, result_column])

  # get treatment names, like placebo and vaccine
  treatment_values = unique(perm_data[, categorical_column])

  # split into 2 groups
  group_1 = perm_data[perm_data[ , categorical_column] == treatment_values[1], result_column]
  group_2 = perm_data[perm_data[ , categorical_column] == treatment_values[2], result_column]
  
  
  # find difference between both groups for comparison to random groupings
  observed_mean_difference = abs(mean(group_1) - mean(group_2))

  # mean difference and counter placeholders
  mean_differences = c()
  counter = 0
  
  # set seed and get number of rows in perm_data
  set.seed(67)
  size = nrow(perm_data)

  for(i in 1:number_permutations){
    indices = sample(seq(1:size), 
                     round( .5 * size))
    sample_1 = perm_data[indices, result_column]
    sample_2 = perm_data[-indices, result_column]
    
    diff = mean(sample_1) - mean(sample_2)
    
    mean_differences[i] = diff
    
    if(abs(diff) > observed_mean_difference){
      counter = counter + 1
    }
    
  }
  
  mean_df = data.frame(mean_differences)
  
  print(ggplot(mean_df, aes(x = mean_differences)) +
    geom_histogram(fill = 'blue', color = 'black') +
    geom_vline(xintercept = observed_mean_difference, color = 'red') +
    labs(title = 'Distribution of the Difference in the Means under Ho',
         x = 'Difference of Means') +
    theme_economist() )
    
  p_value = counter / number_permutations
  
  print(paste('P-value:', format(p_value, scientific = F)))
  
}

permutation_test(df = data, 'OverTime', 'JobSatisfaction', 20000)

x = data %>% filter(OverTime == 'Yes') %>% dplyr::select(JobSatisfaction)
y = data %>% filter(OverTime == 'No') %>% dplyr::select(JobSatisfaction)

wilcox.test(x = as.numeric(x$JobSatisfaction),
            y = as.numeric(y$JobSatisfaction),
            alternative = 'two.sided',
            conf.int = TRUE,
            conf.level = .95)
                
```
If JobSatisfaction rankings were randomly assigned to employees, regardless of whether they worked overtime or not, we would expect to see an difference in means as extreme or more extreme than our observed mean difference 32.9% of the time, suggesting there is not a significant difference in job satisfaction between employees who work overtime and those who do not. A wilcox ranked sum test also failed to reject the null of no difference in centers, median in this case.



